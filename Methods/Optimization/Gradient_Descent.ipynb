{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for computing the gradient of a function f at point x.\n",
    "# The gradient is approximated using the central difference method with a small step size h.\n",
    "def numerical_gradient(f, x, h=1e-4):\n",
    "    grad = np.zeros_like(x)\n",
    "    for i in range(len(x)):\n",
    "        x_minus = np.copy(x)\n",
    "        x_plus = np.copy(x)\n",
    "        x_minus[i] -= h\n",
    "        x_plus[i] += h\n",
    "        grad[i] = (f(x_plus) - f(x_minus)) / (2 * h)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for performing gradient descent on a function f.\n",
    "def gradient_descent(f, x_start, learning_rate=0.1, num_steps=100):\n",
    "    x = x_start.copy()\n",
    "    x_history = [x.copy()]\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= learning_rate * grad  # Update x in the opposite direction of the gradient.\n",
    "        x_history.append(x.copy())\n",
    "\n",
    "    return x, x_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the function and initial point.\n",
    "f = lambda x: x[0]**2 + x[1]**2\n",
    "x = np.array([10.0, 10.0])\n",
    "\n",
    "# Compute the gradient using the function.\n",
    "print(\"Gradient at x =\", x, \":\", numerical_gradient(f, x))\n",
    "\n",
    "final_x, x_history = gradient_descent(f, x, learning_rate=0.1, num_steps=50)\n",
    "\n",
    "print(\"Final x:\", np.round(final_x, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extended plot function that additionally marks the gradient descent trajectory and the optimum.\n",
    "def plot_function_3d(f, x1_range=(-5, 5), x2_range=(-5, 5), num_points=400, x_history=None, optimum=None):\n",
    "    # Generate a grid in the specified range.\n",
    "    x1_vals = np.linspace(x1_range[0], x1_range[1], num_points)\n",
    "    x2_vals = np.linspace(x2_range[0], x2_range[1], num_points)\n",
    "    X1, X2 = np.meshgrid(x1_vals, x2_vals)\n",
    "    \n",
    "    # Vectorize the function if necessary.\n",
    "    f_vec = np.vectorize(lambda x1, x2: f(np.array([x1, x2])))\n",
    "    Z = f_vec(X1, X2)\n",
    "    \n",
    "    # Create the 3D plot.\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    surface = ax.plot_surface(X1, X2, Z, cmap='viridis', alpha=0.8)\n",
    "    ax.set_xlabel(\"x1\")\n",
    "    ax.set_ylabel(\"x2\")\n",
    "    ax.set_zlabel(\"f(x)\")\n",
    "    ax.set_title(\"3D Plot of f(x1, x2)\")\n",
    "    fig.colorbar(surface, shrink=0.5, aspect=5)\n",
    "    \n",
    "    # If the gradient descent trajectory is provided, plot it:\n",
    "    if x_history is not None:\n",
    "        x_history = np.array(x_history)\n",
    "        xs = x_history[:, 0]\n",
    "        ys = x_history[:, 1]\n",
    "        zs = np.array([f(np.array([xi, yi])) for xi, yi in zip(xs, ys)])\n",
    "        ax.scatter(xs, ys, zs, color='red', marker='o', s=50, label='Gradient Descent')\n",
    "        ax.plot(xs, ys, zs, color='red', linewidth=2)\n",
    "    \n",
    "    # If the optimum is provided, plot it:\n",
    "    if optimum is not None:\n",
    "        opt_x = optimum[0]\n",
    "        opt_y = optimum[1]\n",
    "        opt_z = f(np.array([opt_x, opt_y]))\n",
    "        ax.scatter([opt_x], [opt_y], [opt_z], color='blue', marker='o', s=50, label='Optimum')\n",
    "    \n",
    "    ax.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The optimum is automatically provided as final_x from the gradient descent.\n",
    "plot_function_3d(f, x1_range=(-10, 10), x2_range=(-10, 10), x_history=x_history, optimum=final_x)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
