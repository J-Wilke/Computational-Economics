{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sp\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from IPython.display import display, Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Optimization\n",
    "\n",
    "**Optimization** is the process of finding the best solution to a problem from a set of possible solutions. In mathematical terms, we seek to find the minimum or maximum of a function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "- **Objective Function**: The function $f(x)$ we want to minimize or maximize\n",
    "- **Decision Variables**: The variables $x = (x_1, x_2, ..., x_n)$ we can control\n",
    "- **Optimal Solution**: The point $x^*$ where $f(x^*)$ is minimal (or maximal)\n",
    "\n",
    "### Types of Optimization:\n",
    "\n",
    "1. **Unconstrained Optimization**: Find $\\min_{x \\in \\mathbb{R}^n} f(x)$\n",
    "2. **Constrained Optimization**: Find $\\min_{x \\in \\mathbb{R}^n} f(x)$ subject to constraints $g_i(x) \\leq 0$, $h_j(x) = 0$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mathematical Foundations\n",
    "\n",
    "### Gradient\n",
    "\n",
    "The **gradient** of a function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ at a point $x$ is the vector of partial derivatives:\n",
    "\n",
    "$$\\nabla f(x) = \\left[\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, ..., \\frac{\\partial f}{\\partial x_n}\\right]^T$$\n",
    "\n",
    "**Intuition**: The gradient points in the direction of steepest increase of the function. Its negative points in the direction of steepest decrease.\n",
    "\n",
    "### Hessian Matrix\n",
    "\n",
    "The **Hessian matrix** is the matrix of second-order partial derivatives:\n",
    "\n",
    "$$H(x) = \\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**Intuition**: The Hessian describes the curvature of the function, helping us determine whether a point is a minimum, maximum, or saddle point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stationary Point Analysis\n",
    "\n",
    "A **stationary point** (or critical point) is a point where the gradient equals zero:\n",
    "\n",
    "$$\\nabla f(x^*) = 0$$\n",
    "\n",
    "### Classification of Stationary Points\n",
    "\n",
    "Using the Hessian matrix $H(x^*)$ at a stationary point:\n",
    "\n",
    "1. **Local Minimum**: $H(x^*)$ is positive definite (all eigenvalues > 0)\n",
    "2. **Local Maximum**: $H(x^*)$ is negative definite (all eigenvalues < 0)\n",
    "3. **Saddle Point**: $H(x^*)$ has both positive and negative eigenvalues\n",
    "4. **Inconclusive**: $H(x^*)$ is singular or has zero eigenvalues\n",
    "\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define symbols for symbolic computation\n",
    "x, y = sp.symbols('x y')\n",
    "\n",
    "# Example function: f(x,y) = x^2 + y^2\n",
    "f_symbolic = x**2 + y**2\n",
    "print(\"Function: f(x,y) =\", f_symbolic)\n",
    "\n",
    "# Compute gradient symbolically\n",
    "gradient = [sp.diff(f_symbolic, x), sp.diff(f_symbolic, y)]\n",
    "print(\"\\nGradient: ∇f =\", gradient)\n",
    "\n",
    "# Solve for stationary points\n",
    "stationary_points = sp.solve(gradient, (x, y), dict=True)\n",
    "print(\"\\nStationary Points:\", stationary_points)\n",
    "\n",
    "# Compute and display Hessian matrix\n",
    "Hessian = [[sp.diff(f_symbolic, x, x), sp.diff(f_symbolic, x, y)],\n",
    "           [sp.diff(f_symbolic, y, x), sp.diff(f_symbolic, y, y)]]\n",
    "print(\"\\nHessian Matrix:\")\n",
    "display(Math(sp.latex(sp.Matrix(Hessian))))\n",
    "\n",
    "# Classify each stationary point\n",
    "for point in stationary_points:\n",
    "    # Evaluate Hessian at the point\n",
    "    H_eval = [[expr.subs(point) for expr in row] for row in Hessian]\n",
    "    H_matrix = sp.Matrix(H_eval)\n",
    "    \n",
    "    # Calculate eigenvalues\n",
    "    eigenvalues = list(H_matrix.eigenvals().keys())\n",
    "    eigenvalues_float = [float(sp.N(ev, chop=True)) for ev in eigenvalues]\n",
    "    \n",
    "    # Classify based on eigenvalues\n",
    "    if all(ev > 0 for ev in eigenvalues_float):\n",
    "        classification = \"Local Minimum (convex)\"\n",
    "    elif all(ev < 0 for ev in eigenvalues_float):\n",
    "        classification = \"Local Maximum (concave)\"\n",
    "    elif any(ev > 0 for ev in eigenvalues_float) and any(ev < 0 for ev in eigenvalues_float):\n",
    "        classification = \"Saddle Point\"\n",
    "    else:\n",
    "        classification = \"Inconclusive\"\n",
    "    \n",
    "    print(f\"\\nPoint {point}:\")\n",
    "    print(f\"Classification: {classification}\")\n",
    "    print(f\"Eigenvalues: {eigenvalues_float}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Descent Algorithm\n",
    "\n",
    "**Gradient Descent** is an iterative optimization algorithm for finding a local minimum of a differentiable function.\n",
    "\n",
    "### Formal Definition\n",
    "\n",
    "Let $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ be a continuously differentiable function. The gradient descent algorithm generates a sequence $\\{x_k\\}_{k=0}^{\\infty}$ according to the iteration:\n",
    "\n",
    "$$x_{k+1} = x_k - \\alpha_k \\nabla f(x_k)$$\n",
    "\n",
    "where:\n",
    "- $x_0 \\in \\mathbb{R}^n$ is the initial point\n",
    "- $\\alpha_k > 0$ is the **step size** (or **learning rate**) at iteration $k$\n",
    "- $\\nabla f(x_k) \\in \\mathbb{R}^n$ is the gradient of $f$ evaluated at $x_k$\n",
    "\n",
    "### Motivation\n",
    "\n",
    "The gradient descent algorithm is based on the first-order Taylor approximation:\n",
    "\n",
    "$$f(x + \\Delta x) \\approx f(x) + \\nabla f(x)^T \\Delta x$$\n",
    "\n",
    "To minimize $f$, we want to choose $\\Delta x$ such that $f(x + \\Delta x) < f(x)$. The direction of steepest descent is:\n",
    "\n",
    "$$\\Delta x = -\\alpha \\nabla f(x)$$\n",
    "\n",
    "for some $\\alpha > 0$. This gives us the maximum decrease in $f$ for small steps.\n",
    "\n",
    "### Finite Difference Approximation\n",
    "\n",
    "In practice, we often cannot compute $\\nabla f$ analytically. We use the **finite difference method** to approximate partial derivatives:\n",
    "\n",
    "#### Forward Difference:\n",
    "$$\\frac{\\partial f}{\\partial x_i} \\approx \\frac{f(x + h e_i) - f(x)}{h}$$\n",
    "\n",
    "#### Central Difference (more accurate):\n",
    "$$\\frac{\\partial f}{\\partial x_i} \\approx \\frac{f(x + h e_i) - f(x - h e_i)}{2h}$$\n",
    "\n",
    "where:\n",
    "- $h > 0$ is a small step size (typically $h \\in [10^{-8}, 10^{-4}]$)\n",
    "- $e_i$ is the $i$-th standard basis vector\n",
    "\n",
    "\n",
    "### Learning Rate Properties\n",
    "\n",
    "The choice of learning rate $\\alpha_k$ is crucial:\n",
    "\n",
    "1. **Constant Learning Rate**: $\\alpha_k = \\alpha$ for all $k$\n",
    "   - Simple but may not converge or converge slowly\n",
    "   \n",
    "2. **Diminishing Learning Rate**: $\\alpha_k \\rightarrow 0$ as $k \\rightarrow \\infty$\n",
    "   - Ensures convergence under certain conditions\n",
    "   - Example: $\\alpha_k = \\frac{\\alpha_0}{1 + \\beta k}$\n",
    "\n",
    "3. **Adaptive Learning Rate**: Based on local properties of $f$\n",
    "   - Line search methods\n",
    "   - Trust region methods\n",
    "\n",
    "### Stopping Criteria\n",
    "\n",
    "Common criteria for terminating the algorithm:\n",
    "\n",
    "1. **Gradient norm**: $\\|\\nabla f(x_k)\\| < \\varepsilon$\n",
    "2. **Relative change in function value**: $\\frac{|f(x_{k+1}) - f(x_k)|}{|f(x_k)|} < \\varepsilon$\n",
    "3. **Relative change in parameters**: $\\frac{\\|x_{k+1} - x_k\\|}{\\|x_k\\|} < \\varepsilon$\n",
    "4. **Maximum iterations**: $k \\geq k_{\\max}$\n",
    "\n",
    "### Challenges and Limitations\n",
    "\n",
    "1. **Local Minima**: Gradient descent only guarantees convergence to local minima\n",
    "2. **Saddle Points**: Can get stuck or slow down significantly\n",
    "3. **Ill-Conditioning**: Poor performance when Hessian has large condition number\n",
    "4. **Step Size Selection**: Too large → divergence; Too small → slow convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, point, h=1e-4):\n",
    "    \"\"\"\n",
    "    Compute numerical gradient using finite differences.\n",
    "    \n",
    "    Parameters:\n",
    "    - f: function to differentiate\n",
    "    - point: point at which to evaluate gradient\n",
    "    - h: small step for finite difference\n",
    "    \n",
    "    Returns:\n",
    "    - gradient vector\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(point)\n",
    "    for i in range(len(point)):\n",
    "        point_minus = np.copy(point)\n",
    "        point_plus = np.copy(point)\n",
    "        point_minus[i] -= h\n",
    "        point_plus[i] += h\n",
    "        grad[i] = (f(point_plus) - f(point_minus)) / (2 * h)\n",
    "    return grad\n",
    "\n",
    "def gradient_descent(f, start_point, learning_rate=0.1, num_steps=100, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Perform gradient descent optimization.\n",
    "    \n",
    "    Parameters:\n",
    "    - f: objective function\n",
    "    - start_point: initial point\n",
    "    - learning_rate: step size\n",
    "    - num_steps: maximum number of iterations\n",
    "    - tolerance: convergence tolerance\n",
    "    \n",
    "    Returns:\n",
    "    - final_point: optimized point\n",
    "    - history: trajectory of points\n",
    "    \"\"\"\n",
    "    point = start_point.copy()\n",
    "    history = [point.copy()]\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        grad = numerical_gradient(f, point)\n",
    "        \n",
    "        # Check convergence\n",
    "        if np.linalg.norm(grad) < tolerance:\n",
    "            print(f\"Converged after {step} iterations\")\n",
    "            break\n",
    "            \n",
    "        # Update point\n",
    "        point -= learning_rate * grad\n",
    "        history.append(point.copy())\n",
    "    \n",
    "    return point, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting point: [10. 10.]\n",
      "Initial function value: 200.0\n",
      "Initial gradient: [20. 20.]\n",
      "\n",
      "Final point: [0.000143 0.000143]\n",
      "Final function value: 4.0740719526791846e-08\n",
      "Final gradient: [0.00028545 0.00028545]\n"
     ]
    }
   ],
   "source": [
    "# Example: Minimize f(x,y) = x^2 + y^2\n",
    "f_numpy = lambda p: p[0]**2 + p[1]**2\n",
    "start_point = np.array([10.0, 10.0])\n",
    "\n",
    "print(\"Starting point:\", start_point)\n",
    "print(\"Initial function value:\", f_numpy(start_point))\n",
    "print(\"Initial gradient:\", numerical_gradient(f_numpy, start_point))\n",
    "\n",
    "# Run gradient descent\n",
    "final_point, history = gradient_descent(f_numpy, start_point, learning_rate=0.1, num_steps=50)\n",
    "\n",
    "print(\"\\nFinal point:\", np.round(final_point, 6))\n",
    "print(\"Final function value:\", f_numpy(final_point))\n",
    "print(\"Final gradient:\", numerical_gradient(f_numpy, final_point))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison: Analytical vs Numerical Methods\n",
    "\n",
    "### Stationary Point Analysis (Analytical)\n",
    "\n",
    "**Advantages:**\n",
    "- Exact solutions\n",
    "- Provides complete characterization (min/max/saddle)\n",
    "- No iterations needed\n",
    "\n",
    "**Disadvantages:**\n",
    "- Requires differentiable functions\n",
    "- May be difficult or impossible for complex functions\n",
    "- Solving $\\nabla f = 0$ can be computationally hard\n",
    "\n",
    "### Gradient Descent (Numerical)\n",
    "\n",
    "**Advantages:**\n",
    "- Works for any differentiable function\n",
    "- Can handle high-dimensional problems\n",
    "- Doesn't require solving equations\n",
    "\n",
    "**Disadvantages:**\n",
    "- Only finds local minima\n",
    "- Requires hyperparameter tuning (learning rate)\n",
    "- May converge slowly or not at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizations and Examples\n",
    "\n",
    "Let's visualize both methods on various functions to understand their behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gradient_descent_visualization(f_numpy, f_symbolic, x_range=(-5, 5), y_range=(-5, 5), \n",
    "                                start_points=None, num_points=100):\n",
    "    \"\"\"\n",
    "    Visualize gradient descent paths and stationary points on a 2D function.\n",
    "    \"\"\"\n",
    "    # Create meshgrid\n",
    "    x_vals = np.linspace(x_range[0], x_range[1], num_points)\n",
    "    y_vals = np.linspace(y_range[0], y_range[1], num_points)\n",
    "    X, Y = np.meshgrid(x_vals, y_vals)\n",
    "    \n",
    "    # Evaluate function\n",
    "    f_vec = np.vectorize(lambda x, y: f_numpy(np.array([x, y])))\n",
    "    Z = f_vec(X, Y)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # 3D Surface plot\n",
    "    ax1 = fig.add_subplot(131, projection='3d')\n",
    "    surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, edgecolor='none')\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    ax1.set_zlabel('f(x,y)')\n",
    "    ax1.set_title('3D Surface with Gradient Descent Paths')\n",
    "    \n",
    "    # Find and plot stationary points\n",
    "    x_sym, y_sym = sp.symbols('x y')\n",
    "    gradient_sym = [sp.diff(f_symbolic, x_sym), sp.diff(f_symbolic, y_sym)]\n",
    "    stationary_pts = sp.solve(gradient_sym, (x_sym, y_sym), dict=True)\n",
    "    \n",
    "    for pt in stationary_pts:\n",
    "        if all(val.is_real for val in pt.values()):\n",
    "            x_val = float(pt[x_sym])\n",
    "            y_val = float(pt[y_sym])\n",
    "            z_val = f_numpy(np.array([x_val, y_val]))\n",
    "            ax1.scatter([x_val], [y_val], [z_val], color='red', s=100, marker='*')\n",
    "    \n",
    "    # Gradient descent trajectories\n",
    "    if start_points is not None:\n",
    "        for start in start_points:\n",
    "            _, history = gradient_descent(f_numpy, start, learning_rate=0.1, num_steps=50)\n",
    "            history = np.array(history)\n",
    "            xs = history[:, 0]\n",
    "            ys = history[:, 1]\n",
    "            zs = np.array([f_numpy(np.array([x, y])) for x, y in zip(xs, ys)])\n",
    "            ax1.plot(xs, ys, zs, 'r-', linewidth=2, alpha=0.8)\n",
    "            ax1.scatter(xs[0], ys[0], zs[0], color='green', s=50, marker='o')\n",
    "    \n",
    "    # Contour plot\n",
    "    ax2 = fig.add_subplot(132)\n",
    "    contours = ax2.contour(X, Y, Z, levels=20, cmap='viridis')\n",
    "    ax2.clabel(contours, inline=True, fontsize=8)\n",
    "    ax2.set_xlabel('x')\n",
    "    ax2.set_ylabel('y')\n",
    "    ax2.set_title('Contour Plot with Gradient Descent Paths')\n",
    "    \n",
    "    # Plot stationary points\n",
    "    for pt in stationary_pts:\n",
    "        if all(val.is_real for val in pt.values()):\n",
    "            x_val = float(pt[x_sym])\n",
    "            y_val = float(pt[y_sym])\n",
    "            ax2.plot(x_val, y_val, 'r*', markersize=15, label='Stationary Point')\n",
    "    \n",
    "    # Plot gradient descent paths\n",
    "    if start_points is not None:\n",
    "        for i, start in enumerate(start_points):\n",
    "            _, history = gradient_descent(f_numpy, start, learning_rate=0.1, num_steps=50)\n",
    "            history = np.array(history)\n",
    "            ax2.plot(history[:, 0], history[:, 1], 'r-', alpha=0.8)\n",
    "            ax2.plot(history[0, 0], history[0, 1], 'go', markersize=8)\n",
    "            ax2.plot(history[-1, 0], history[-1, 1], 'ro', markersize=8)\n",
    "    \n",
    "    \n",
    "    # Gradient vector field\n",
    "    ax3 = fig.add_subplot(133)\n",
    "    \n",
    "    # Sample gradient at fewer points for clarity\n",
    "    x_grad = np.linspace(x_range[0], x_range[1], 20)\n",
    "    y_grad = np.linspace(y_range[0], y_range[1], 20)\n",
    "    X_grad, Y_grad = np.meshgrid(x_grad, y_grad)\n",
    "    \n",
    "    # Compute gradients\n",
    "    U = np.zeros_like(X_grad)\n",
    "    V = np.zeros_like(Y_grad)\n",
    "    for i in range(len(x_grad)):\n",
    "        for j in range(len(y_grad)):\n",
    "            grad = numerical_gradient(f_numpy, np.array([X_grad[j,i], Y_grad[j,i]]))\n",
    "            U[j,i] = -grad[0]  # Negative for descent direction\n",
    "            V[j,i] = -grad[1]\n",
    "    \n",
    "    # Normalize for better visualization\n",
    "    N = np.sqrt(U**2 + V**2)\n",
    "    U_norm = U / (N + 1e-10)\n",
    "    V_norm = V / (N + 1e-10)\n",
    "    \n",
    "    ax3.quiver(X_grad, Y_grad, U_norm, V_norm, N, cmap='coolwarm', alpha=0.8)\n",
    "    ax3.contour(X, Y, Z, levels=10, colors='gray', alpha=0.3)\n",
    "    ax3.set_xlabel('x')\n",
    "    ax3.set_ylabel('y')\n",
    "    ax3.set_title('Gradient Descent Direction Field')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Simple quadratic function\n",
    "print(\"Example 1: Quadratic Function f(x,y) = x² + y²\")\n",
    "f1_numpy = lambda p: p[0]**2 + p[1]**2\n",
    "f1_symbolic = x**2 + y**2\n",
    "start_points_1 = [np.array([4.0, 3.0]), np.array([-3.0, 4.0]), np.array([2.0, -4.0])]\n",
    "\n",
    "plot_gradient_descent_visualization(f1_numpy, f1_symbolic, x_range=(-5, 5), y_range=(-5, 5), \n",
    "                           start_points=start_points_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
